{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "coupled-willow",
   "metadata": {},
   "source": [
    "### Collecting Data - Lab 3\n",
    "#### Inter-annotator agreement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "consolidated-protection",
   "metadata": {},
   "source": [
    "In this exercise, you will calculate inter-annotator agreement using Cohen's kappa, both manually and using a ready-made Python function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "oriented-credit",
   "metadata": {},
   "source": [
    "Imagine that you have two annotators labelling text sentiment, where the labels are \"positive\", \"neutral\", or \"negative\". The two lists below provide the annotators' labels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "outside-montana",
   "metadata": {},
   "outputs": [],
   "source": [
    "annotator_a = [\"positive\",\n",
    "              \"neutral\",\n",
    "              \"negative\",\n",
    "              \"negative\",\n",
    "              \"positive\",\n",
    "              \"neutral\",\n",
    "              \"positive\",\n",
    "              \"positive\",\n",
    "              \"neutral\",\n",
    "              \"neutral\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "occasional-instrument",
   "metadata": {},
   "outputs": [],
   "source": [
    "annotator_b = [\"neutral\",\n",
    "              \"neutral\",\n",
    "              \"negative\",\n",
    "              \"negative\",\n",
    "              \"positive\",\n",
    "              \"neutral\",\n",
    "              \"neutral\",\n",
    "              \"positive\",\n",
    "              \"neutral\",\n",
    "              \"positive\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "periodic-store",
   "metadata": {},
   "source": [
    "Of course, Python has a function to compute Cohen's kappa automatically. It's provided in the Scikit-learn library as specified below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tropical-israeli",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import cohen_kappa_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "according-terrorism",
   "metadata": {},
   "outputs": [],
   "source": [
    "kappa_automatic = cohen_kappa_score(annotator_a, annotator_b)\n",
    "print(kappa_automatic)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "particular-bracelet",
   "metadata": {},
   "source": [
    "Now you actually know the answer. What can you say about the inter-annotator agreement in this case? Look at the last few slides in Lecture 3 to interpret the resulting kappa value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "happy-anthropology",
   "metadata": {},
   "source": [
    "-----------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "guilty-enforcement",
   "metadata": {},
   "source": [
    "Next, the idea is to calculate Cohen's kappa manually. By going through the calculations step-by-step, you will be able to understand how it's actually computed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "destroyed-dynamics",
   "metadata": {},
   "source": [
    "Recall from the lecture that there are 4 steps to compute the value of Cohen's kappa:\n",
    "1. Build a confusion matrix from the annotations.\n",
    "2. Compute the raw (observed) agreement - a numeric variable `agreement_obs`.\n",
    "3. Compute the expected agreement - a numeric variable `agreement_exp`.\n",
    "5. Compute Cohen's kappa: $kappa = \\frac{(agreement\\_obs - agreement\\_exp)}{(1 - agreement\\_exp)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "twenty-sweden",
   "metadata": {},
   "source": [
    "Before getting to the computations, we define a couple of useful variables below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "billion-slovakia",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total number of items:\n",
    "n_items = len(annotator_a)\n",
    "\n",
    "# Labels:\n",
    "annotation_labels = [\"positive\", \"neutral\", \"negative\"]\n",
    "\n",
    "# Number of labels:\n",
    "n_labels = len(annotation_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tropical-measurement",
   "metadata": {},
   "source": [
    "#### Step 1. Confusion matrix.\n",
    "To build a confusion matrix, you can use another function from the Scikit-learn library, `confusion_matrix`. Write 2-3 lines of code to import the function, compute, and print a confusion matrix for the annotations provided in the two lists above.\n",
    "\n",
    "**Important**. In this case, you need to provide the `labels` argument to the `confusion_matrix` function for it to work properly. If needed, consult the documentation:\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intellectual-unemployment",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the function:\n",
    "\n",
    "# Compute a confusion matrix:\n",
    "\n",
    "# Print the matrix:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "consolidated-crazy",
   "metadata": {},
   "source": [
    "#### Step 2. Observed agreement.\n",
    "Observed agreement is computed by looking only at those cases where the two annotators agree. All such cases appear in the _diagonal_ of the confusion matrix (top left to bottom right).\n",
    "\n",
    "To extract the diagonal from a matrix, you can use the `diag` function (without any arguments except your matrix) from the `numpy` package. If needed, consult the documentation:\n",
    "https://numpy.org/doc/stable/reference/generated/numpy.diag.html\n",
    "\n",
    "Write code to import the function, extract the diagonal into a list, sum all the list elements into `num_agreed_items` and compute the observed agreement: $agreement\\_obs = \\frac{num\\_agreed\\_items}{total\\_items}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "published-eligibility",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the function:\n",
    "\n",
    "# Extract the diagonal:\n",
    "\n",
    "# Compute num_agreed_items, the number of items agreed upon:\n",
    "\n",
    "# Compute the observed agreement:\n",
    "\n",
    "# Print the observed agreement:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "protected-clearing",
   "metadata": {},
   "source": [
    "#### Step 3. Expected agreement.\n",
    "Expected agreement is computed per label. In your confusion matrix, rows represent annotator A and columns annotator B. Each annotator can choose 1 of the 3 labels - therefore, you have 3 rows and 3 columns.\n",
    "\n",
    "To compute the expected agreement for a particular label, you need to multiply two values:\n",
    "1) total number of times annotator **A** chooses that label, divided by the total number of items;\n",
    "2) total number of times annotator **B** chooses that label, divided by the total number of items.\n",
    "\n",
    "To compute (1), you can add up the values in a particular row of the matrix `sum(matrix[label_idx,:])` and divide that by `n_items`.\n",
    "\n",
    "To compute (2), you add up the values in a particular column: `sum(matrix[:, label_idx])` and again divide that by `n_items`.\n",
    "\n",
    "By multiplying these two values, you get the expected agreement for a given label. If you do that in a loop and add up the values for each label, you get the total expected agreement value, `agreement_exp`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incomplete-hungarian",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define an empty list agreement_exp_per_label:\n",
    "\n",
    "# Set agreement_exp to be 0:\n",
    "\n",
    "# Use a for-loop (label_idx from 0 to n_labels):\n",
    "\n",
    "    # Compute agreement_exp_per_label:\n",
    "\n",
    "    # Add agreement_exp_per_label to agreement_exp:\n",
    "\n",
    "# Print agreement_exp \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "better-assistant",
   "metadata": {},
   "source": [
    "#### Step 5. Cohen's kappa.\n",
    "\n",
    "All that's left to do is to supply your computed values of `agreement_obs` and `agreement_exp` to the formula defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "geographic-friendly",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute kappa:\n",
    "\n",
    "# Print kappa:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spanish-trial",
   "metadata": {},
   "source": [
    "Let's compare your kappa value to the one computed automatically using a Scikit-learn function. If the cell below returns True, you've got it right, otherwise you need to look back and find where you made a mistake."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "opposed-owner",
   "metadata": {},
   "outputs": [],
   "source": [
    "kappa_automatic == kappa"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
