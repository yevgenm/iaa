{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "scenic-finding",
   "metadata": {},
   "source": [
    "### Collecting Data - Lab 3\n",
    "#### Inter-annotator agreement (solutions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stock-grammar",
   "metadata": {},
   "source": [
    "In this exercise, you will calculate inter-annotator agreement using Cohen's kappa, both manually and using a ready-made Python function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "engaged-smell",
   "metadata": {},
   "source": [
    "Imagine that you have two annotators labelling text sentiment, where the labels are \"positive\", \"neutral\", or \"negative\". The two lists below provide the annotators' labels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "geographic-momentum",
   "metadata": {},
   "outputs": [],
   "source": [
    "annotator_a = [\"positive\",\n",
    "              \"neutral\",\n",
    "              \"negative\",\n",
    "              \"negative\",\n",
    "              \"positive\",\n",
    "              \"neutral\",\n",
    "              \"positive\",\n",
    "              \"positive\",\n",
    "              \"neutral\",\n",
    "              \"neutral\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "simplified-lesson",
   "metadata": {},
   "outputs": [],
   "source": [
    "annotator_b = [\"neutral\",\n",
    "              \"neutral\",\n",
    "              \"negative\",\n",
    "              \"negative\",\n",
    "              \"positive\",\n",
    "              \"neutral\",\n",
    "              \"neutral\",\n",
    "              \"positive\",\n",
    "              \"neutral\",\n",
    "              \"positive\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "athletic-oxford",
   "metadata": {},
   "source": [
    "Of course, Python has a function to compute Cohen's kappa automatically. It's provided in the Scikit-learn library as specified below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "historic-silence",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import cohen_kappa_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "muslim-isaac",
   "metadata": {},
   "outputs": [],
   "source": [
    "kappa_automatic = cohen_kappa_score(annotator_a, annotator_b)\n",
    "print(kappa_automatic)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "foreign-processing",
   "metadata": {},
   "source": [
    "Now you actually know the answer. What can you say about the inter-annotator agreement in this case? Look at the last few slides in Lecture 3 to interpret the resulting kappa value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "raising-haven",
   "metadata": {},
   "source": [
    "-----------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "steady-arthritis",
   "metadata": {},
   "source": [
    "Next, the idea is to calculate Cohen's kappa manually. By going through the calculations step-by-step, you will be able to understand how it's actually computed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "early-anderson",
   "metadata": {},
   "source": [
    "Recall from the lecture that there are 4 steps to compute the value of Cohen's kappa:\n",
    "1. Build a confusion matrix from the annotations.\n",
    "2. Compute the raw (observed) agreement - a numeric variable `agreement_obs`.\n",
    "3. Compute the expected agreement - a numeric variable `agreement_exp`.\n",
    "5. Compute Cohen's kappa: $kappa = \\frac{(agreement\\_obs - agreement\\_exp)}{(1 - agreement\\_exp)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "understanding-democrat",
   "metadata": {},
   "source": [
    "Before getting to the computations, we define a couple of useful variables below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chicken-fault",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total number of items:\n",
    "n_items = len(annotator_a)\n",
    "\n",
    "# Labels:\n",
    "annotation_labels = [\"positive\", \"neutral\", \"negative\"]\n",
    "\n",
    "# Number of labels:\n",
    "n_labels = len(annotation_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "northern-height",
   "metadata": {},
   "source": [
    "#### Step 1. Confusion matrix.\n",
    "To build a confusion matrix, you can use another function from the Scikit-learn library, `confusion_matrix`. Write 2-3 lines of code to import the function, compute, and print a confusion matrix for the annotations provided in the two lists above.\n",
    "\n",
    "**Important**. In this case, you need to provide the `labels` argument to the `confusion_matrix` function for it to work properly. If needed, consult the documentation:\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "proved-celtic",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the function:\n",
    "from sklearn.metrics import confusion_matrix\n",
    "# Compute a confusion matrix:\n",
    "matrix = confusion_matrix(annotator_a, annotator_b, labels=annotation_labels)\n",
    "# Print the matrix:\n",
    "print(matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "surprised-calculator",
   "metadata": {},
   "source": [
    "#### Step 2. Observed agreement.\n",
    "Observed agreement is computed by looking only at those cases where the two annotators agree. All such cases appear in the _diagonal_ of the confusion matrix (top left to bottom right).\n",
    "\n",
    "To extract the diagonal from a matrix, you can use the `diag` function (without any arguments except your matrix) from the `numpy` package. If needed, consult the documentation:\n",
    "https://numpy.org/doc/stable/reference/generated/numpy.diag.html\n",
    "\n",
    "Write code to import the function, extract the diagonal into a list, sum all the list elements into `num_agreed_items` and compute the observed agreement: $agreement\\_obs = \\frac{num\\_agreed\\_items}{total\\_items}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "color-integral",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the function:\n",
    "from numpy import diag\n",
    "# Extract the diagonal:\n",
    "d = diag(matrix)\n",
    "# Compute num_agreed_items, the number of items agreed upon:\n",
    "num_agreed_items = sum(d)\n",
    "# Compute the observed agreement:\n",
    "agreement_obs = num_agreed_items/n_items\n",
    "# Print the observed agreement:\n",
    "print(agreement_obs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "similar-giant",
   "metadata": {},
   "source": [
    "#### Step 3. Expected agreement.\n",
    "Expected agreement is computed per label. In your confusion matrix, rows represent annotator A and columns annotator B. Each annotator can choose 1 of the 3 labels - therefore, you have 3 rows and 3 columns.\n",
    "\n",
    "To compute the expected agreement for a particular label, you need to multiply two values:\n",
    "1) total number of times annotator **A** chooses that label, divided by the total number of items;\n",
    "2) total number of times annotator **B** chooses that label, divided by the total number of items.\n",
    "\n",
    "To compute (1), you can add up the values in a particular row of the matrix `sum(matrix[label_idx,:])` and divide that by `n_items`.\n",
    "\n",
    "To compute (2), you add up the values in a particular column: `sum(matrix[:, label_idx])` and again divide that by `n_items`.\n",
    "\n",
    "By multiplying these two values, you get the expected agreement for a given label. If you do that in a loop and add up the values for each label, you get the total expected agreement value, `agreement_exp`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unlikely-health",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define an empty list agreement_exp_per_label:\n",
    "agreement_exp_per_label = []\n",
    "\n",
    "# Set agreement_exp to be 0:\n",
    "agreement_exp = 0\n",
    "\n",
    "# Use a for-loop (label_idx from 0 to n_labels):\n",
    "for label_idx in range(n_labels):\n",
    "    # Compute agreement_exp_per_label:\n",
    "    agreement_exp_per_label = sum(matrix[label_idx,:])/n_items * sum(matrix[:, label_idx])/n_items\n",
    "    # Add agreement_exp_per_label to agreement_exp:\n",
    "    agreement_exp = agreement_exp + agreement_exp_per_label\n",
    "    \n",
    "print(agreement_exp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "parental-basement",
   "metadata": {},
   "source": [
    "#### Step 5. Cohen's kappa.\n",
    "\n",
    "All that's left to do is to supply your computed values of `agreement_obs` and `agreement_exp` to the formula defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "further-scroll",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute kappa:\n",
    "kappa = (agreement_obs - agreement_exp) / (1 - agreement_exp)\n",
    "# Print kappa:\n",
    "print(kappa)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "russian-rwanda",
   "metadata": {},
   "source": [
    "Let's compare your kappa value to the one computed automatically using a Scikit-learn function. If the cell below returns True, you've got it right, otherwise you need to look back and find where you made a mistake."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "existing-large",
   "metadata": {},
   "outputs": [],
   "source": [
    "kappa_automatic == kappa"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
